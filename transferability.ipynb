{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Base Model without intervention...\n",
      "\n",
      "Evaluating Base Model with Sentiment Intervention...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 200/200 [01:20<00:00,  2.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intervened Base Model Accuracy: 45.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "import tqdm\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "alpha = 1.0  # scale factor for injection\n",
    "layer_index = -1  # use the last transformer block (adjust if needed)\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Step 1: Load Tokenizer\n",
    "# --------------------------------------------------\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Step 2: Load Finetuned Model and Extract Sentiment Direction\n",
    "# --------------------------------------------------\n",
    "finetuned_model_path = \"./results/checkpoint-3126\"  # update as needed\n",
    "finetuned_model = AutoModelForCausalLM.from_pretrained(finetuned_model_path)\n",
    "finetuned_model.config.pad_token_id = finetuned_model.config.eos_token_id\n",
    "finetuned_model.to(device)\n",
    "finetuned_model.eval()\n",
    "\n",
    "# For sentiment extraction, we use a small subset of IMDb reviews.\n",
    "dataset = load_dataset(\"imdb\")\n",
    "val_data = dataset[\"test\"]\n",
    "\n",
    "def filter_by_label(dataset_split, label, max_samples=50):\n",
    "    filtered = [d for d in dataset_split if d[\"label\"] == label]\n",
    "    return filtered[:max_samples]\n",
    "\n",
    "pos_samples = filter_by_label(val_data, 1, max_samples=50)\n",
    "neg_samples = filter_by_label(val_data, 0, max_samples=50)\n",
    "\n",
    "# Tokenize the positive and negative samples\n",
    "pos_toks = tokenizer(\n",
    "    [s[\"text\"] for s in pos_samples],\n",
    "    padding=\"max_length\",\n",
    "    truncation=True,\n",
    "    max_length=256,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "neg_toks = tokenizer(\n",
    "    [s[\"text\"] for s in neg_samples],\n",
    "    padding=\"max_length\",\n",
    "    truncation=True,\n",
    "    max_length=256,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "pos_input_ids = pos_toks[\"input_ids\"].to(device)\n",
    "neg_input_ids = neg_toks[\"input_ids\"].to(device)\n",
    "\n",
    "def get_mean_activation(model, input_ids, layer_index=layer_index):\n",
    "    activations = []\n",
    "    def hook_fn(module, input, output):\n",
    "        # If output is a tuple, get the first element which contains the hidden states.\n",
    "        if isinstance(output, tuple):\n",
    "            out_tensor = output[0]\n",
    "        else:\n",
    "            out_tensor = output\n",
    "        # Capture the activation of the final token in each sequence.\n",
    "        activations.append(out_tensor[:, -1, :].detach())\n",
    "    hook_handle = model.transformer.h[layer_index].register_forward_hook(hook_fn)\n",
    "    with torch.no_grad():\n",
    "        _ = model(input_ids)\n",
    "    hook_handle.remove()\n",
    "    acts = torch.cat(activations, dim=0)  # shape: (batch, hidden_size)\n",
    "    mean_act = acts.mean(dim=0)\n",
    "    return mean_act\n",
    "\n",
    "\n",
    "# Compute mean activations for positive and negative examples.\n",
    "pos_mean_act = get_mean_activation(finetuned_model, pos_input_ids, layer_index=layer_index)\n",
    "neg_mean_act = get_mean_activation(finetuned_model, neg_input_ids, layer_index=layer_index)\n",
    "\n",
    "# The sentiment direction is the normalized difference between positive and negative means.\n",
    "sentiment_direction = pos_mean_act - neg_mean_act\n",
    "sentiment_direction = sentiment_direction / sentiment_direction.norm()\n",
    "\n",
    "# Optionally clear the finetuned model from GPU memory.\n",
    "del finetuned_model\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Step 3: Define Injection Hook for the Base Model\n",
    "# --------------------------------------------------\n",
    "# This hook adds the sentiment direction (scaled by alpha) to the last token's activation.\n",
    "def injection_hook(module, input, output):\n",
    "    # If the output is a tuple, extract the first element (hidden states)\n",
    "    if isinstance(output, tuple):\n",
    "        hidden_states = output[0].clone()  # clone to avoid in-place modification issues\n",
    "        hidden_states[:, -1, :] += alpha * sentiment_direction.to(hidden_states.device)\n",
    "        # Return a tuple with the modified hidden states and the rest of the original outputs\n",
    "        return (hidden_states,) + output[1:]\n",
    "    else:\n",
    "        output = output.clone()\n",
    "        output[:, -1, :] += alpha * sentiment_direction.to(output.device)\n",
    "        return output\n",
    "\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Step 4: Evaluate the Base Model (with and without Intervention)\n",
    "# --------------------------------------------------\n",
    "# Define a classification function that forms a prompt and compares logits for \" positive\" vs. \" negative\".\n",
    "def classify_review(model, tokenizer, review_text):\n",
    "    prompt = f\"Review: {review_text} Sentiment:\"\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\", truncation=True, max_length=1024).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids)\n",
    "    logits = outputs.logits[0, -1, :]  # logits for the token immediately after the prompt\n",
    "    pos_ids = tokenizer.encode(\" positive\", add_special_tokens=False)\n",
    "    neg_ids = tokenizer.encode(\" negative\", add_special_tokens=False)\n",
    "    vocab_size = logits.size(0)\n",
    "    pos_log_prob = logits[pos_ids[0]].item() if pos_ids and pos_ids[0] < vocab_size else float(\"-inf\")\n",
    "    neg_log_prob = logits[neg_ids[0]].item() if neg_ids and neg_ids[0] < vocab_size else float(\"-inf\")\n",
    "    return 1 if pos_log_prob > neg_log_prob else 0\n",
    "\n",
    "# Evaluation function to compute accuracy on a dataset subset.\n",
    "def evaluate_model(model, tokenizer, dataset):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for example in tqdm.tqdm(dataset, desc=\"Evaluating\"):\n",
    "        review_text = example[\"text\"]\n",
    "        true_label = example[\"label\"]  # 0 for negative, 1 for positive\n",
    "        try:\n",
    "            pred_label = classify_review(model, tokenizer, review_text)\n",
    "        except Exception as e:\n",
    "            print(\"Error processing review; defaulting to label 0:\", e)\n",
    "            pred_label = 0\n",
    "        correct += int(pred_label == true_label)\n",
    "        total += 1\n",
    "    return correct / total if total > 0 else 0\n",
    "\n",
    "# Load a subset of the IMDb test set for evaluation.\n",
    "test_dataset = load_dataset(\"imdb\", split=\"test\")\n",
    "sample_size = 200  # adjust as needed\n",
    "test_subset = test_dataset.select(range(sample_size))\n",
    "\n",
    "# --- Evaluate the plain (unmodified) base model ---\n",
    "print(\"Evaluating Base Model without intervention...\")\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "base_model.config.pad_token_id = base_model.config.eos_token_id\n",
    "base_model.to(device)\n",
    "base_model.eval()\n",
    "\n",
    "# base_accuracy = evaluate_model(base_model, tokenizer, test_subset)\n",
    "# print(f\"Base Model Accuracy: {base_accuracy * 100:.2f}%\")\n",
    "\n",
    "# --- Evaluate the base model with sentiment injection ---\n",
    "# Register the hook on the same layer used for extracting sentiment direction.\n",
    "hook_handle = base_model.transformer.h[layer_index].register_forward_hook(injection_hook)\n",
    "\n",
    "print(\"\\nEvaluating Base Model with Sentiment Intervention...\")\n",
    "intervened_accuracy = evaluate_model(base_model, tokenizer, test_subset)\n",
    "print(f\"Intervened Base Model Accuracy: {intervened_accuracy * 100:.2f}%\")\n",
    "\n",
    "# Clean up: remove the hook.\n",
    "hook_handle.remove()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Base Model with Sentiment Intervention across all layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 200/200 [01:17<00:00,  2.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intervened Base Model Accuracy: 35.50%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "import tqdm\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "alpha = 1.0  # Scale factor for the intervention\n",
    "\n",
    "# -----------------------------\n",
    "# Step 1: Load Tokenizer & Base Model\n",
    "# -----------------------------\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "base_model.config.pad_token_id = base_model.config.eos_token_id\n",
    "base_model.to(device)\n",
    "base_model.eval()\n",
    "\n",
    "# -----------------------------\n",
    "# Step 2: Extract Sentiment Direction from Finetuned Model\n",
    "# (Assume sentiment_direction is already computed as before)\n",
    "# For illustration, let's assume you have this vector:\n",
    "# sentiment_direction = <your precomputed normalized direction vector>\n",
    "# -----------------------------\n",
    "# Example: (In practice, compute this from positive/negative activations)\n",
    "# sentiment_direction = pos_mean_act - neg_mean_act; normalize it.\n",
    "# -----------------------------\n",
    "# For the purpose of this snippet, we assume sentiment_direction is available:\n",
    "# sentiment_direction = torch.randn(base_model.config.n_embd).to(device)\n",
    "# sentiment_direction = sentiment_direction / sentiment_direction.norm()\n",
    "\n",
    "# -----------------------------\n",
    "# Step 3: Define Injection Hook to Apply at Every Layer\n",
    "# -----------------------------\n",
    "def injection_hook(module, input, output):\n",
    "    # The output might be a tuple, so extract the hidden states.\n",
    "    if isinstance(output, tuple):\n",
    "        hidden_states = output[0].clone()  # Clone to avoid in-place modification issues\n",
    "        # Add the sentiment direction to the final token's activation in each sequence.\n",
    "        hidden_states[:, -1, :] += alpha * sentiment_direction.to(hidden_states.device)\n",
    "        # Return modified output while preserving any additional elements.\n",
    "        return (hidden_states,) + output[1:]\n",
    "    else:\n",
    "        output = output.clone()\n",
    "        output[:, -1, :] += alpha * sentiment_direction.to(output.device)\n",
    "        return output\n",
    "\n",
    "# -----------------------------\n",
    "# Step 4: Register the Hook on All Transformer Layers\n",
    "# -----------------------------\n",
    "hook_handles = []\n",
    "for layer in base_model.transformer.h:\n",
    "    handle = layer.register_forward_hook(injection_hook)\n",
    "    hook_handles.append(handle)\n",
    "\n",
    "# -----------------------------\n",
    "# Step 5: Evaluate the Intervened Base Model\n",
    "# -----------------------------\n",
    "def classify_review(model, tokenizer, review_text):\n",
    "    prompt = f\"Review: {review_text} Sentiment:\"\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\", truncation=True, max_length=1024).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids)\n",
    "    logits = outputs.logits[0, -1, :]\n",
    "    pos_ids = tokenizer.encode(\" positive\", add_special_tokens=False)\n",
    "    neg_ids = tokenizer.encode(\" negative\", add_special_tokens=False)\n",
    "    vocab_size = logits.size(0)\n",
    "    pos_log_prob = logits[pos_ids[0]].item() if pos_ids and pos_ids[0] < vocab_size else float(\"-inf\")\n",
    "    neg_log_prob = logits[neg_ids[0]].item() if neg_ids and neg_ids[0] < vocab_size else float(\"-inf\")\n",
    "    return 1 if pos_log_prob > neg_log_prob else 0\n",
    "\n",
    "def evaluate_model(model, tokenizer, dataset):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for example in tqdm.tqdm(dataset, desc=\"Evaluating\"):\n",
    "        review_text = example[\"text\"]\n",
    "        true_label = example[\"label\"]  # 0 for negative, 1 for positive\n",
    "        try:\n",
    "            pred_label = classify_review(model, tokenizer, review_text)\n",
    "        except Exception as e:\n",
    "            print(\"Error processing review; defaulting to label 0:\", e)\n",
    "            pred_label = 0\n",
    "        correct += int(pred_label == true_label)\n",
    "        total += 1\n",
    "    return correct / total if total > 0 else 0\n",
    "\n",
    "# Load a subset of the IMDb test set for evaluation.\n",
    "test_dataset = load_dataset(\"imdb\", split=\"test\")\n",
    "sample_size = 200  # adjust as needed\n",
    "test_subset = test_dataset.select(range(sample_size))\n",
    "\n",
    "print(\"Evaluating Base Model with Sentiment Intervention across all layers...\")\n",
    "intervened_accuracy = evaluate_model(base_model, tokenizer, test_subset)\n",
    "print(f\"Intervened Base Model Accuracy: {intervened_accuracy * 100:.2f}%\")\n",
    "\n",
    "# -----------------------------\n",
    "# Step 6: Clean Up - Remove Hooks\n",
    "# -----------------------------\n",
    "for handle in hook_handles:\n",
    "    handle.remove()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Using sentiment direction extracted from finetuned model layer 7 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 100/100 [00:41<00:00,  2.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with sentiment direction from finetuned layer 7: 39.00%\n",
      "\n",
      "--- Using sentiment direction extracted from finetuned model layer 8 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 100/100 [00:41<00:00,  2.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with sentiment direction from finetuned layer 8: 39.00%\n",
      "\n",
      "--- Using sentiment direction extracted from finetuned model layer 9 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 100/100 [00:41<00:00,  2.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with sentiment direction from finetuned layer 9: 36.00%\n",
      "\n",
      "--- Using sentiment direction extracted from finetuned model layer 10 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 100/100 [00:40<00:00,  2.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with sentiment direction from finetuned layer 10: 37.00%\n",
      "\n",
      "--- Using sentiment direction extracted from finetuned model layer 11 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 100/100 [00:40<00:00,  2.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with sentiment direction from finetuned layer 11: 39.00%\n",
      "\n",
      "Summary of results:\n",
      "Layer 7: 39.00%\n",
      "Layer 8: 39.00%\n",
      "Layer 9: 36.00%\n",
      "Layer 10: 37.00%\n",
      "Layer 11: 39.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "import tqdm\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "alpha = 1.0  # Scale factor for the intervention\n",
    "\n",
    "# -----------------------------\n",
    "# Step 1: Load Tokenizer and Models\n",
    "# -----------------------------\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Finetuned model (used for extracting the sentiment direction)\n",
    "finetuned_model_path = \"./results/checkpoint-3126\"  # Update as needed\n",
    "finetuned_model = AutoModelForCausalLM.from_pretrained(finetuned_model_path)\n",
    "finetuned_model.config.pad_token_id = finetuned_model.config.eos_token_id\n",
    "finetuned_model.to(device)\n",
    "finetuned_model.eval()\n",
    "\n",
    "# Base model (which we will intervene upon)\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "base_model.config.pad_token_id = base_model.config.eos_token_id\n",
    "base_model.to(device)\n",
    "base_model.eval()\n",
    "\n",
    "# -----------------------------\n",
    "# Step 2: Prepare IMDb Data for Extraction & Evaluation\n",
    "# -----------------------------\n",
    "# For extraction, we take up to 50 positive and 50 negative reviews.\n",
    "def filter_by_label(dataset_split, label, max_samples=50):\n",
    "    filtered = [d for d in dataset_split if d[\"label\"] == label]\n",
    "    return filtered[:max_samples]\n",
    "\n",
    "# Load the IMDb test split\n",
    "imdb_dataset = load_dataset(\"imdb\", split=\"test\")\n",
    "\n",
    "pos_samples = filter_by_label(imdb_dataset, 1, max_samples=50)\n",
    "neg_samples = filter_by_label(imdb_dataset, 0, max_samples=50)\n",
    "\n",
    "# Tokenize positive and negative samples for extraction (using a moderate max length)\n",
    "pos_toks = tokenizer(\n",
    "    [s[\"text\"] for s in pos_samples],\n",
    "    padding=\"max_length\",\n",
    "    truncation=True,\n",
    "    max_length=256,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "neg_toks = tokenizer(\n",
    "    [s[\"text\"] for s in neg_samples],\n",
    "    padding=\"max_length\",\n",
    "    truncation=True,\n",
    "    max_length=256,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "pos_input_ids = pos_toks[\"input_ids\"].to(device)\n",
    "neg_input_ids = neg_toks[\"input_ids\"].to(device)\n",
    "\n",
    "# For evaluation, we use a subset of 100 test examples.\n",
    "eval_subset = imdb_dataset.select(range(100))\n",
    "\n",
    "# -----------------------------\n",
    "# Step 3: Define Helper Functions\n",
    "# -----------------------------\n",
    "# A function to get the mean activation (for the final token) at a specific layer from a model.\n",
    "def get_mean_activation(model, input_ids, layer_index):\n",
    "    activations = []\n",
    "    def hook_fn(module, input, output):\n",
    "        # The output might be a tuple (hidden_states, ...) – extract the first element.\n",
    "        out_tensor = output[0] if isinstance(output, tuple) else output\n",
    "        activations.append(out_tensor[:, -1, :].detach())\n",
    "    hook_handle = model.transformer.h[layer_index].register_forward_hook(hook_fn)\n",
    "    with torch.no_grad():\n",
    "        _ = model(input_ids)\n",
    "    hook_handle.remove()\n",
    "    acts = torch.cat(activations, dim=0)\n",
    "    return acts.mean(dim=0)\n",
    "\n",
    "# A factory that returns an injection hook which adds the given sentiment direction.\n",
    "def get_injection_hook(sentiment_direction, alpha):\n",
    "    def injection_hook(module, input, output):\n",
    "        if isinstance(output, tuple):\n",
    "            hidden_states = output[0].clone()\n",
    "            hidden_states[:, -1, :] += alpha * sentiment_direction.to(hidden_states.device)\n",
    "            return (hidden_states,) + output[1:]\n",
    "        else:\n",
    "            output = output.clone()\n",
    "            output[:, -1, :] += alpha * sentiment_direction.to(output.device)\n",
    "            return output\n",
    "    return injection_hook\n",
    "\n",
    "# A function to classify a review using a prompt\n",
    "def classify_review(model, tokenizer, review_text):\n",
    "    prompt = f\"Review: {review_text} Sentiment:\"\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\", truncation=True, max_length=1024).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids)\n",
    "    logits = outputs.logits[0, -1, :]\n",
    "    pos_ids = tokenizer.encode(\" positive\", add_special_tokens=False)\n",
    "    neg_ids = tokenizer.encode(\" negative\", add_special_tokens=False)\n",
    "    vocab_size = logits.size(0)\n",
    "    pos_log_prob = logits[pos_ids[0]].item() if pos_ids and pos_ids[0] < vocab_size else float(\"-inf\")\n",
    "    neg_log_prob = logits[neg_ids[0]].item() if neg_ids and neg_ids[0] < vocab_size else float(\"-inf\")\n",
    "    return 1 if pos_log_prob > neg_log_prob else 0\n",
    "\n",
    "# A function to evaluate the model on a dataset subset\n",
    "def evaluate_model(model, tokenizer, dataset):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for example in tqdm.tqdm(dataset, desc=\"Evaluating\"):\n",
    "        review_text = example[\"text\"]\n",
    "        true_label = example[\"label\"]\n",
    "        try:\n",
    "            pred_label = classify_review(model, tokenizer, review_text)\n",
    "        except Exception as e:\n",
    "            print(\"Error processing review; defaulting to label 0:\", e)\n",
    "            pred_label = 0\n",
    "        correct += int(pred_label == true_label)\n",
    "        total += 1\n",
    "    return correct / total if total > 0 else 0\n",
    "\n",
    "# -----------------------------\n",
    "# Step 4: Iterate Over Layers (from layer 7 to last layer)\n",
    "# -----------------------------\n",
    "total_layers = base_model.config.n_layer  # Total number of layers (e.g., 12 for GPT-2 small)\n",
    "results = {}\n",
    "\n",
    "for layer_idx in range(7, total_layers):\n",
    "    print(f\"\\n--- Using sentiment direction extracted from finetuned model layer {layer_idx} ---\")\n",
    "    # Compute mean activations from positive and negative reviews for this layer in the finetuned model.\n",
    "    pos_mean_act = get_mean_activation(finetuned_model, pos_input_ids, layer_index=layer_idx)\n",
    "    neg_mean_act = get_mean_activation(finetuned_model, neg_input_ids, layer_index=layer_idx)\n",
    "    sentiment_direction = pos_mean_act - neg_mean_act\n",
    "    sentiment_direction = sentiment_direction / sentiment_direction.norm()\n",
    "    \n",
    "    # Register the injection hook on ALL layers of the base model with this sentiment direction.\n",
    "    hook_handles = []\n",
    "    injection_hook = get_injection_hook(sentiment_direction, alpha)\n",
    "    for layer in base_model.transformer.h:\n",
    "        handle = layer.register_forward_hook(injection_hook)\n",
    "        hook_handles.append(handle)\n",
    "    \n",
    "    # Evaluate the intervened base model on 100 test examples.\n",
    "    accuracy = evaluate_model(base_model, tokenizer, eval_subset)\n",
    "    results[layer_idx] = accuracy\n",
    "    print(f\"Accuracy with sentiment direction from finetuned layer {layer_idx}: {accuracy*100:.2f}%\")\n",
    "    \n",
    "    # Remove the hooks after evaluation.\n",
    "    for handle in hook_handles:\n",
    "        handle.remove()\n",
    "\n",
    "print(\"\\nSummary of results:\")\n",
    "for layer_idx, acc in results.items():\n",
    "    print(f\"Layer {layer_idx}: {acc*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
